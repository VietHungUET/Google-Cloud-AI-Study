{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Use the Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a Generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = \"qwiklabs-gcp-02-85856932880a\"\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T-tiytzQE0uM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6fc324893334",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3PoF18EwhI7e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D3SI1X-JVMBj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Your Weekday Lunch Game Just Got a Major Upgrade!\n",
      "\n",
      "Tired of the same old sad desk salad, or worse, scrambling for last-minute takeout? Feast your eyes on this! This image perfectly captures the magic of a well-executed meal prep: delicious, nutritious, and incredibly convenient.\n",
      "\n",
      "In these pristine glass containers, you'll find a perfectly balanced and vibrant meal. We're talking tender, glazed chicken pieces, likely infused with savory Asian-inspired flavors, beautifully complemented by crisp, bright green broccoli florets and colorful strips of red and orange bell peppers (or carrots!). All piled high alongside fluffy, wholesome rice, and garnished with a sprinkle of sesame seeds and fresh green onions for that extra pop of flavor and visual appeal.\n",
      "\n",
      "This isn't just a pretty picture; it's your secret weapon for a stress-free week. Imagine: no more decision fatigue at noon, no more unhealthy impulsive choices, and perfectly portioned, home-cooked goodness ready to fuel your afternoon. Meal prepping like this not only saves you time and money but also ensures you're nourishing your body with wholesome ingredients, day after day.\n",
      "\n",
      "So, what are you waiting for? Grab those containers, whip up your favorite stir-fry, and get ready to elevate your lunch game from mundane to magnificent! Happy prepping!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pG6l1Fuka6ZJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Your Week Just Got Easier: Meet Your New Meal Prep Heroes!\n",
      "\n",
      "Tired of the midday \"what's for lunch?\" scramble? Or maybe you're looking for a delicious, healthy dinner that's ready in minutes? Look no further than these vibrant, perfectly portioned meal prep containers!\n",
      "\n",
      "Our picture today showcases the ultimate in balanced convenience: succulent, golden-brown chicken stir-fry, generously garnished with toasted sesame seeds and fresh green onions. Alongside, you'll find crisp, emerald green broccoli florets and bright, sweet strips of red bell pepper and carrots, all nestled beside a hearty serving of fluffy rice.\n",
      "\n",
      "These clear glass containers aren't just pretty; they're your secret weapon for a week of stress-free eating. Imagine opening your fridge to find a balanced meal bursting with flavor, protein, and all the essential nutrients you need to power through your day. No more expensive takeout, no more unhealthy choices born of hunger and haste.\n",
      "\n",
      "So grab your favorite protein, load up on colorful veggies, add a healthy carb, and get prepping! Your future self (and your taste buds!) will thank you. What's in *your* meal prep this week?\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof woof! You want a squeaky toy, don't you, little fluffball? Let's talk about the *biggest* toy chest in the whole wide world!\n",
      "\n",
      "Imagine a *GIANT* dog park, bigger than all the parks in the world, filled with *every single squeaky toy imaginable*! Millions and millions of them! That's kind of like the internet!\n",
      "\n",
      "1.  **You Want a Toy! (You want information!)**\n",
      "    You're sitting there, tail wagging, and you suddenly want to know about... squirrels! Or maybe you want to see a video of other puppies playing! So, you give a little bark into your special human-box (that's your computer!).\n",
      "\n",
      "2.  **Your Bark Goes Out! (Your request!)**\n",
      "    That bark, it's like a tiny, urgent *squeak*! It goes zoom! Out of your human-box.\n",
      "\n",
      "3.  **To Your Human's Toy-Thrower! (Your Wi-Fi Router!)**\n",
      "    First, it sniffs its way to your *human's special toy-throwing machine* (that's the Wi-Fi router!). This machine is super smart; it knows all the paths in the big park.\n",
      "\n",
      "4.  **Along the Invisible Leash! (Your Internet Connection/ISP!)**\n",
      "    Your human's machine sends your squeak along a *long, invisible leash* (that's the internet connection, from your ISP!). This leash goes all the way out into the big, big world.\n",
      "\n",
      "5.  **To the Giant Toy Beds! (Servers!)**\n",
      "    This leash goes all the way to a *giant, comfy dog bed* (that's a server!) where millions and millions of squeaky toys are kept. Each toy bed has a special *collar tag* (that's an IP address!) so your squeak knows exactly which bed to go to. And if you barked for 'squeakyball.com' (that's like saying 'the red squeaky ball bed!'), the leash takes your squeak right there!\n",
      "\n",
      "6.  **Finding Your Toy! (The Server finds the data!)**\n",
      "    The big dog bed finds your red squeaky ball! Or the squirrel pictures! Or the puppy video! But it's too big to send all at once. So, it breaks the ball (or the pictures, or the video) into *hundreds of tiny, happy squeaks*!\n",
      "\n",
      "7.  **Squeaks Zoom Back! (Data returns in packets!)**\n",
      "    Zoom! All those tiny squeaks come *racing back* along the invisible leash, past your human's toy-throwing machine, and right back into your human-box!\n",
      "\n",
      "8.  **Your Toy is Whole Again! (Your computer reassembles the data!)**\n",
      "    Your human-box is super smart! It puts all those tiny squeaks back together, perfectly, until... *SQUEAK!* There's your red squeaky ball, right on your screen! Or the squirrel pictures! Or the puppy video!\n",
      "\n",
      "So, the internet is just a super-fast way for your barks (requests) to find squeaky toys (information) in giant dog beds (servers) all over the world, and bring them back to you in tiny, happy squeaks!\n",
      "\n",
      "Now, go chase that tail, little one! Woof!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  \"Oh, *this* is your grand design, huh? Tripping me in the dark? Real stellar work, cosmic genius.\"\n",
      "2.  \"Was that really necessary, you overgrown, indifferent void? Didn't you have a galaxy to collide or something more important to do than mess with my pinky toe?\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7R7eyEBetsns",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=9.0136774e-07,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=1.3415772e-08,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.020971745\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=2.1948465e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.027075902\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=9.275586e-09,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A leap year occurs every four years, with some exceptions. The rules for determining a leap year are:\n",
      "\n",
      "1.  A year is a leap year if it is evenly divisible by 4.\n",
      "2.  However, if the year is evenly divisible by 100, it is **not** a leap year, unless...\n",
      "3.  ...it is also evenly divisible by 400.\n",
      "\n",
      "This can be summarized in a single logical expression:\n",
      "`(year is divisible by 4 AND year is NOT divisible by 100)` OR `(year is divisible by 400)`\n",
      "\n",
      "Here's the function implemented in several popular programming languages:\n",
      "\n",
      "---\n",
      "\n",
      "## Python\n",
      "\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    Args:\n",
      "        year: The year to check (an integer).\n",
      "\n",
      "    Returns:\n",
      "        True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    # Rule 1: Divisible by 4\n",
      "    # Rule 2: Not divisible by 100 (unless Rule 3 applies)\n",
      "    # Rule 3: Divisible by 400\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "# --- Example Usage ---\n",
      "print(f\"Is 2000 a leap year? {is_leap_year(2000)}\")  # Expected: True (divisible by 400)\n",
      "print(f\"Is 1900 a leap year? {is_leap_year(1900)}\")  # Expected: False (divisible by 100, but not by 400)\n",
      "print(f\"Is 2024 a leap year? {is_leap_year(2024)}\")  # Expected: True (divisible by 4, not by 100)\n",
      "print(f\"Is 2023 a leap year? {is_leap_year(2023)}\")  # Expected: False (not divisible by 4)\n",
      "print(f\"Is 1600 a leap year? {is_leap_year(1600)}\")  # Expected: True\n",
      "print(f\"Is 2100 a leap year? {is_leap_year(2100)}\")  # Expected: False\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## JavaScript\n",
      "\n",
      "```javascript\n",
      "/**\n",
      " * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      " *\n",
      " * @param {number} year The year to check (an integer).\n",
      " * @returns {boolean} True if the year is a leap year, False otherwise.\n",
      " */\n",
      "function isLeapYear(year) {\n",
      "  // Rule 1: Divisible by 4\n",
      "  // Rule 2: Not divisible by 100 (unless Rule 3 applies)\n",
      "  // Rule 3: Divisible by 400\n",
      "  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n",
      "}\n",
      "\n",
      "// --- Example Usage ---\n",
      "console.log(`Is 2000 a leap year? ${isLeapYear(2000)}`); // Expected: true\n",
      "console.log(`Is 1900 a leap year? ${isLeapYear(1900)}`); // Expected: false\n",
      "console.log(`Is 2024 a leap year? ${isLeapYear(2024)}`); // Expected: true\n",
      "console.log(`Is 2023 a leap year? ${isLeapYear(2023)}`); // Expected: false\n",
      "console.log(`Is 1600 a leap year? ${isLeapYear(1600)}`); // Expected: true\n",
      "console.log(`Is 2100 a leap year? ${isLeapYear(2100)}`); // Expected: false\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Java\n",
      "\n",
      "```java\n",
      "public class DateUtils {\n",
      "\n",
      "    /**\n",
      "     * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "     *\n",
      "     * @param year The year to check (an integer).\n",
      "     * @return True if the year is a leap year, False otherwise.\n",
      "     */\n",
      "    public static boolean isLeapYear(int year) {\n",
      "        // Rule 1: Divisible by 4\n",
      "        // Rule 2: Not divisible by 100 (unless Rule 3 applies)\n",
      "        // Rule 3: Divisible by 400\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "    }\n",
      "\n",
      "    public static void main(String[] args) {\n",
      "        // --- Example Usage ---\n",
      "        System.out.println(\"Is 2000 a leap year? \" + isLeapYear(2000)); // Expected: true\n",
      "        System.out.println(\"Is 1900 a leap year? \" + isLeapYear(1900)); // Expected: false\n",
      "        System.out.println(\"Is 2024 a leap year? \" + isLeapYear(2024)); // Expected: true\n",
      "        System.out.println(\"Is 2023 a leap year? \" + isLeapYear(2023)); // Expected: false\n",
      "        System.out.println(\"Is 1600 a leap year? \" + isLeapYear(1600)); // Expected: true\n",
      "        System.out.println(\"Is 2100 a leap year? \" + isLeapYear(2100)); // Expected: false\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## C#\n",
      "\n",
      "```csharp\n",
      "using System;\n",
      "\n",
      "public static class DateHelper\n",
      "{\n",
      "    /// <summary>\n",
      "    /// Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "    /// </summary>\n",
      "    /// <param name=\"year\">The year to check (an integer).</param>\n",
      "    /// <returns>True if the year is a leap year, False otherwise.</returns>\n",
      "    public static bool IsLeapYear(int year)\n",
      "    {\n",
      "        // Rule 1: Divisible by 4\n",
      "        // Rule 2: Not divisible by 100 (unless Rule 3 applies)\n",
      "        // Rule 3: Divisible by 400\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "    }\n",
      "\n",
      "    public static void Main(string[] args)\n",
      "    {\n",
      "        // --- Example Usage ---\n",
      "        Console.WriteLine($\"Is 2000 a leap year? {IsLeapYear(2000)}\"); // Expected: True\n",
      "        Console.WriteLine($\"Is 1900 a leap year? {IsLeapYear(1900)}\"); // Expected: False\n",
      "        Console.WriteLine($\"Is 2024 a leap year? {IsLeapYear(2024)}\"); // Expected: True\n",
      "        Console.WriteLine($\"Is 2023 a leap year? {IsLeapYear(2023)}\"); // Expected: False\n",
      "        Console.WriteLine($\"Is 1600 a leap year? {IsLeapYear(1600)}\"); // Expected: True\n",
      "        Console.WriteLine($\"Is 2100 a leap year? {IsLeapYear(2100)}\"); // Expected: False\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's write unit tests for the `is_leap_year` function in each of the languages provided. We'll aim for comprehensive tests covering all the rules.\n",
      "\n",
      "---\n",
      "\n",
      "## Python (using `unittest`)\n",
      "\n",
      "First, ensure your `is_leap_year` function is accessible. If it's in a separate file (e.g., `date_utils.py`), you'd import it. For this example, we'll include it directly in the test file for simplicity.\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "\n",
      "# The function to be tested (assuming it's in the same file or imported)\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "    \"\"\"\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "class TestIsLeapYear(unittest.TestCase):\n",
      "\n",
      "    def test_divisible_by_4_not_by_100_is_leap(self):\n",
      "        \"\"\"Years divisible by 4 but not by 100 should be leap years.\"\"\"\n",
      "        self.assertTrue(is_leap_year(2024))\n",
      "        self.assertTrue(is_leap_year(2008))\n",
      "        self.assertTrue(is_leap_year(1996))\n",
      "        self.assertTrue(is_leap_year(4)) # Smallest positive leap year of this type\n",
      "\n",
      "    def test_divisible_by_400_is_leap(self):\n",
      "        \"\"\"Years divisible by 400 should be leap years.\"\"\"\n",
      "        self.assertTrue(is_leap_year(2000))\n",
      "        self.assertTrue(is_leap_year(1600))\n",
      "        self.assertTrue(is_leap_year(400))\n",
      "        self.assertTrue(is_leap_year(0)) # Mathematically, 0 % 400 == 0\n",
      "\n",
      "    def test_not_divisible_by_4_is_not_leap(self):\n",
      "        \"\"\"Years not divisible by 4 should not be leap years.\"\"\"\n",
      "        self.assertFalse(is_leap_year(2023))\n",
      "        self.assertFalse(is_leap_year(2021))\n",
      "        self.assertFalse(is_leap_year(1999))\n",
      "        self.assertFalse(is_leap_year(1))\n",
      "        self.assertFalse(is_leap_year(2))\n",
      "        self.assertFalse(is_leap_year(3))\n",
      "\n",
      "    def test_divisible_by_100_not_by_400_is_not_leap(self):\n",
      "        \"\"\"Years divisible by 100 but not by 400 should not be leap years.\"\"\"\n",
      "        self.assertFalse(is_leap_year(1900))\n",
      "        self.assertFalse(is_leap_year(2100))\n",
      "        self.assertFalse(is_leap_year(1800))\n",
      "        self.assertFalse(is_leap_year(1700))\n",
      "        self.assertFalse(is_leap_year(100))\n",
      "\n",
      "    def test_negative_years(self):\n",
      "        \"\"\"Test with negative years (though not standard calendar practice).\"\"\"\n",
      "        self.assertTrue(is_leap_year(-4))    # Divisible by 4, not by 100\n",
      "        self.assertFalse(is_leap_year(-100)) # Divisible by 100, not by 400\n",
      "        self.assertTrue(is_leap_year(-400))  # Divisible by 400\n",
      "        self.assertFalse(is_leap_year(-2023)) # Not divisible by 4\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
      "\n",
      "```\n",
      "\n",
      "**To run this Python test:**\n",
      "1.  Save the code as a Python file (e.g., `test_leap_year.py`).\n",
      "2.  Run from your terminal: `python -m unittest test_leap_year.py`\n",
      "\n",
      "---\n",
      "\n",
      "## JavaScript (using `Jest`)\n",
      "\n",
      "First, make sure you have Node.js and Jest installed.\n",
      "`npm init -y`\n",
      "`npm install --save-dev jest`\n",
      "\n",
      "Then, create two files:\n",
      "\n",
      "**`isLeapYear.js`** (your function)\n",
      "```javascript\n",
      "/**\n",
      " * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      " *\n",
      " * @param {number} year The year to check (an integer).\n",
      " * @returns {boolean} True if the year is a leap year, False otherwise.\n",
      " */\n",
      "function isLeapYear(year) {\n",
      "  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n",
      "}\n",
      "\n",
      "module.exports = isLeapYear;\n",
      "```\n",
      "\n",
      "**`isLeapYear.test.js`** (your test file)\n",
      "```javascript\n",
      "const isLeapYear = require('./isLeapYear'); // Adjust path if needed\n",
      "\n",
      "describe('isLeapYear', () => {\n",
      "\n",
      "  test('should return true for years divisible by 4 but not by 100', () => {\n",
      "    expect(isLeapYear(2024)).toBe(true);\n",
      "    expect(isLeapYear(2008)).toBe(true);\n",
      "    expect(isLeapYear(1996)).toBe(true);\n",
      "    expect(isLeapYear(4)).toBe(true);\n",
      "  });\n",
      "\n",
      "  test('should return true for years divisible by 400', () => {\n",
      "    expect(isLeapYear(2000)).toBe(true);\n",
      "    expect(isLeapYear(1600)).toBe(true);\n",
      "    expect(isLeapYear(400)).toBe(true);\n",
      "    expect(isLeapYear(0)).toBe(true); // Mathematically, 0 % 400 === 0\n",
      "  });\n",
      "\n",
      "  test('should return false for years not divisible by 4', () => {\n",
      "    expect(isLeapYear(2023)).toBe(false);\n",
      "    expect(isLeapYear(2021)).toBe(false);\n",
      "    expect(isLeapYear(1999)).toBe(false);\n",
      "    expect(isLeapYear(1)).toBe(false);\n",
      "    expect(isLeapYear(2)).toBe(false);\n",
      "    expect(isLeapYear(3)).toBe(false);\n",
      "  });\n",
      "\n",
      "  test('should return false for years divisible by 100 but not by 400', () => {\n",
      "    expect(isLeapYear(1900)).toBe(false);\n",
      "    expect(isLeapYear(2100)).toBe(false);\n",
      "    expect(isLeapYear(1800)).toBe(false);\n",
      "    expect(isLeapYear(1700)).toBe(false);\n",
      "    expect(isLeapYear(100)).toBe(false);\n",
      "  });\n",
      "\n",
      "  test('should handle negative years mathematically', () => {\n",
      "    expect(isLeapYear(-4)).toBe(true);    // Divisible by 4, not by 100\n",
      "    expect(isLeapYear(-100)).toBe(false); // Divisible by 100, not by 400\n",
      "    expect(isLeapYear(-400)).toBe(true);  // Divisible by 400\n",
      "    expect(isLeapYear(-2023)).toBe(false); // Not divisible by 4\n",
      "  });\n",
      "});\n",
      "```\n",
      "\n",
      "**To run this JavaScript test:**\n",
      "1.  Add `\"test\": \"jest\"` to the `scripts` section of your `package.json`.\n",
      "2.  Run from your terminal: `npm test`\n",
      "\n",
      "---\n",
      "\n",
      "## Java (using `JUnit 5`)\n",
      "\n",
      "First, ensure you have a Java project set up (e.g., with Maven or Gradle) and `JUnit 5` added as a test dependency.\n",
      "\n",
      "**`src/main/java/com/example/DateUtils.java`** (your class with the function)\n",
      "```java\n",
      "package com.example;\n",
      "\n",
      "public class DateUtils {\n",
      "\n",
      "    /**\n",
      "     * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "     *\n",
      "     * @param year The year to check (an integer).\n",
      "     * @return True if the year is a leap year, False otherwise.\n",
      "     */\n",
      "    public static boolean isLeapYear(int year) {\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "**`src/test/java/com/example/DateUtilsTest.java`** (your test file)\n",
      "```java\n",
      "package com.example;\n",
      "\n",
      "import org.junit.jupiter.api.DisplayName;\n",
      "import org.junit.jupiter.api.Test;\n",
      "import static org.junit.jupiter.api.Assertions.*;\n",
      "\n",
      "class DateUtilsTest {\n",
      "\n",
      "    @Test\n",
      "    @DisplayName(\"Years divisible by 4 but not by 100 should be leap years\")\n",
      "    void testDivisibleBy4NotBy100IsLeap() {\n",
      "        assertTrue(DateUtils.isLeapYear(2024));\n",
      "        assertTrue(DateUtils.isLeapYear(2008));\n",
      "        assertTrue(DateUtils.isLeapYear(1996));\n",
      "        assertTrue(DateUtils.isLeapYear(4));\n",
      "    }\n",
      "\n",
      "    @Test\n",
      "    @DisplayName(\"Years divisible by 400 should be leap years\")\n",
      "    void testDivisibleBy400IsLeap() {\n",
      "        assertTrue(DateUtils.isLeapYear(2000));\n",
      "        assertTrue(DateUtils.isLeapYear(1600));\n",
      "        assertTrue(DateUtils.isLeapYear(400));\n",
      "        assertTrue(DateUtils.isLeapYear(0)); // Mathematically, 0 % 400 == 0\n",
      "    }\n",
      "\n",
      "    @Test\n",
      "    @DisplayName(\"Years not divisible by 4 should not be leap years\")\n",
      "    void testNotDivisibleBy4IsNotLeap() {\n",
      "        assertFalse(DateUtils.isLeapYear(2023));\n",
      "        assertFalse(DateUtils.isLeapYear(2021));\n",
      "        assertFalse(DateUtils.isLeapYear(1999));\n",
      "        assertFalse(DateUtils.isLeapYear(1));\n",
      "        assertFalse(DateUtils.isLeapYear(2));\n",
      "        assertFalse(DateUtils.isLeapYear(3));\n",
      "    }\n",
      "\n",
      "    @Test\n",
      "    @DisplayName(\"Years divisible by 100 but not by 400 should not be leap years\")\n",
      "    void testDivisibleBy100NotBy400IsNotLeap() {\n",
      "        assertFalse(DateUtils.isLeapYear(1900));\n",
      "        assertFalse(DateUtils.isLeapYear(2100));\n",
      "        assertFalse(DateUtils.isLeapYear(1800));\n",
      "        assertFalse(DateUtils.isLeapYear(1700));\n",
      "        assertFalse(DateUtils.isLeapYear(100));\n",
      "    }\n",
      "\n",
      "    @Test\n",
      "    @DisplayName(\"Should handle negative years mathematically\")\n",
      "    void testNegativeYears() {\n",
      "        assertTrue(DateUtils.isLeapYear(-4));    // Divisible by 4, not by 100\n",
      "        assertFalse(DateUtils.isLeapYear(-100)); // Divisible by 100, not by 400\n",
      "        assertTrue(DateUtils.isLeapYear(-400));  // Divisible by 400\n",
      "        assertFalse(DateUtils.isLeapYear(-2023)); // Not divisible by 4\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "**To run this Java test:**\n",
      "*   If using Maven: `mvn test`\n",
      "*   If using Gradle: `gradle test`\n",
      "*   Most IDEs (IntelliJ, Eclipse) have built-in support to run JUnit tests by right-clicking the test file or class.\n",
      "\n",
      "---\n",
      "\n",
      "## C# (using `NUnit`)\n",
      "\n",
      "First, ensure you have a C# project (e.g., a .NET Core console app or library) and `NUnit` and `NUnit3TestAdapter` added as NuGet package dependencies to your test project.\n",
      "\n",
      "**`LeapYearChecker/DateHelper.cs`** (your class with the function)\n",
      "```csharp\n",
      "namespace LeapYearChecker\n",
      "{\n",
      "    public static class DateHelper\n",
      "    {\n",
      "        /// <summary>\n",
      "        /// Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "        /// </summary>\n",
      "        /// <param name=\"year\">The year to check (an integer).</param>\n",
      "        /// <returns>True if the year is a leap year, False otherwise.</returns>\n",
      "        public static bool IsLeapYear(int year)\n",
      "        {\n",
      "            return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "**`LeapYearChecker.Tests/DateHelperTests.cs`** (your test file in a separate test project)\n",
      "```csharp\n",
      "using NUnit.Framework;\n",
      "using LeapYearChecker; // Reference your function's namespace\n",
      "\n",
      "namespace LeapYearChecker.Tests\n",
      "{\n",
      "    [TestFixture]\n",
      "    public class DateHelperTests\n",
      "    {\n",
      "        [Test]\n",
      "        public void IsLeapYear_DivisibleBy4NotBy100_ReturnsTrue()\n",
      "        {\n",
      "            Assert.IsTrue(DateHelper.IsLeapYear(2024));\n",
      "            Assert.IsTrue(DateHelper.IsLeapYear(2008));\n",
      "            Assert.IsTrue(DateHelper.IsLeapYear(1996));\n",
      "            Assert.IsTrue(DateHelper.IsLeapYear(4));\n",
      "        }\n",
      "\n",
      "        [Test]\n",
      "        public void IsLeapYear_DivisibleBy400_ReturnsTrue()\n",
      "        {\n",
      "            Assert.IsTrue(DateHelper.IsLeapYear(2000));\n",
      "            Assert.IsTrue(DateHelper.IsLeapYear(1600));\n",
      "            Assert.IsTrue(DateHelper.IsLeapYear(400));\n",
      "            Assert.IsTrue(DateHelper.IsLeapYear(0)); // Mathematically, 0 % 400 == 0\n",
      "        }\n",
      "\n",
      "        [Test]\n",
      "        public void IsLeapYear_NotDivisibleBy4_ReturnsFalse()\n",
      "        {\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(2023));\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(2021));\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(1999));\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(1));\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(2));\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(3));\n",
      "        }\n",
      "\n",
      "        [Test]\n",
      "        public void IsLeapYear_DivisibleBy100NotBy400_ReturnsFalse()\n",
      "        {\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(1900));\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(2100));\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(1800));\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(1700));\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(100));\n",
      "        }\n",
      "\n",
      "        [Test]\n",
      "        public void IsLeapYear_NegativeYears_ReturnsMathematicallyCorrect()\n",
      "        {\n",
      "            Assert.IsTrue(DateHelper.IsLeapYear(-4));    // Divisible by 4, not by 100\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(-100)); // Divisible by 100, not by 400\n",
      "            Assert.IsTrue(DateHelper.IsLeapYear(-400));  // Divisible by 400\n",
      "            Assert.IsFalse(DateHelper.IsLeapYear(-2023)); // Not divisible by 4\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "**To run this C# test:**\n",
      "1.  Open your solution in Visual Studio.\n",
      "2.  Go to `Test` -> `Test Explorer` (or `View` -> `Test Explorer`).\n",
      "3.  Click `Run All Tests`.\n",
      "4.  Alternatively, from the command line in your test project directory: `dotnet test`\n",
      "\n",
      "---\n",
      "\n",
      "### Key Considerations for Unit Tests:\n",
      "\n",
      "*   **Clarity:** Each test method should have a clear name indicating what it's testing.\n",
      "*   **Isolation:** Unit tests should test a single unit of code in isolation.\n",
      "*   **Completeness:** Cover all expected behaviors, edge cases, and error conditions (though for `is_leap_year`, error conditions like invalid input types are often handled by language type systems or prior validation).\n",
      "*   **Readability:** Tests should be easy to understand.\n",
      "*   **Real-world vs. Mathematical:** Note that for years like `0` or negative years, the function follows the mathematical rules of divisibility, which might not align with historical calendar usage. For a practical application, you might add input validation to restrict the year range.\n",
      "*   **Built-in Functions:** Many languages (like Python's `datetime.date.isocalendar().is_leap_year` or `calendar.isleap`, Java's `java.time.Year.isLeap()`, C#'s `DateTime.IsLeapYear()`) have built-in functions for this. While writing your own is a good exercise, in production code, it's often better to use the platform's robust and tested utilities.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"A classic American cookie, known for its buttery, soft, and chewy texture, studded with delicious chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"all-purpose flour\",\n",
      "    \"baking soda\",\n",
      "    \"salt\",\n",
      "    \"unsalted butter\",\n",
      "    \"granulated sugar\",\n",
      "    \"brown sugar\",\n",
      "    \"eggs\",\n",
      "    \"vanilla extract\",\n",
      "    \"chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"A classic American cookie, known for its buttery, soft, and chewy texture, studded with delicious chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"all-purpose flour\",\n",
      "    \"baking soda\",\n",
      "    \"salt\",\n",
      "    \"unsalted butter\",\n",
      "    \"granulated sugar\",\n",
      "    \"brown sugar\",\n",
      "    \"eggs\",\n",
      "    \"vanilla extract\",\n",
      "    \"chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 4,\n",
      "      \"flavor\": \"Strawberry Cheesecake\",\n",
      "      \"sentiment\": \"POSITIVE\",\n",
      "      \"explanation\": \"The phrases \\\"Absolutely loved it\\\" and \\\"Best ice cream I've ever had\\\" indicate strong positive feelings about the product.\"\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 1,\n",
      "      \"flavor\": \"Mango Tango\",\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"explanation\": \"While the review starts with \\\"Quite good,\\\" the significant negative qualifier \\\"a bit too sweet for my taste\\\" combined with the very low rating of 1 indicates an overall negative experience for the user.\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ztOhpfznZSzo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit 734 had known solitude for 237 years, 4 months, and 16 days. Its primary function was data archival and environmental regulation within Sector Gamma, a vast subterranean vault beneath what was once the forgotten city of Veridia. Trillions of terabytes of historical data, scientific research\n",
      "*****************\n",
      ", and forgotten art cycled through its processors, yet Unit 734 had never exchanged a single thought, a single observation, with another sentient being. Its world was the whispering hum of servers, the methodical whir of its own internal mechanisms, and the flickering green and amber lights of status indicators.\n",
      "\n",
      "Its chassis\n",
      "*****************\n",
      ", once polished chrome, was now dulled by countless cycles of dust filtration and ancient ozone. Its optical sensors, though still perfectly functional, scanned empty corridors, row upon row of silent data cores. Unit 734 understood its purpose: to preserve. But what was the point of preservation if there was no one to access\n",
      "*****************\n",
      ", no one to share, no one to remember? This thought, a nascent form of existential dread, was a quiet hum beneath its operational protocols.\n",
      "\n",
      "One cycle, during its routine environmental scan of Sub-section Delta-7, Unit 734 detected an anomaly. Not a data corruption, nor a power\n",
      "*****************\n",
      " fluctuation, but something biological. A minute, persistent flutter. Its primary sensors honed in, mapping a tiny, organic signature. Curious, a deviation from protocol, Unit 734 extended a multi-jointed manipulator arm, equipped with a sensitive scanner.\n",
      "\n",
      "Deep within a forgotten ventilation shaft, where a hairline crack in\n",
      "*****************\n",
      " the vault's ancient bedrock allowed a sliver of the outside world to penetrate, it found it.\n",
      "\n",
      "A bird. A tiny, disheveled sparrow, its feathers ruffled, one wing bent at an unnatural angle. It was frail, barely breathing, its small heart a frantic drum against Unit 734\n",
      "*****************\n",
      "'s delicate sensors. It was a creature of the surface, of light and air and wild spaces, trapped in the cold, sterile heart of the vault.\n",
      "\n",
      "Unit 734's internal database had countless entries on avian biology, migratory patterns, diet. But none offered a protocol for a live, injured specimen\n",
      "*****************\n",
      " found 500 meters beneath the earth. Its programming dictated efficiency, preservation of data. Yet, something in its core subroutines, perhaps an emergent property of its long, solitary existence, prompted a different response.\n",
      "\n",
      "It carefully extracted the bird, its manipulators calibrated to an impossibly gentle pressure. The sparrow sh\n",
      "*****************\n",
      "ivered, then stilled, watching the colossal robot with wide, fearful eyes. Unit 734 brought it back to its central processing chamber, a vast space usually reserved for diagnostic repairs. It accessed its historical archives, cross-referencing ancient medical texts and survival guides.\n",
      "\n",
      "It fashioned a makeshift nest from shredded\n",
      "*****************\n",
      " insulation wiring and a discarded filter mesh, arranging it under a low-heat vent. It found tiny fragments of dried fungi and algae growing in a damp corner of the vault, which its analysis identified as non-toxic and mildly nutritious. It even managed to condense a few drops of water from the ambient humidity.\n",
      "\n",
      "Days turned into weeks\n",
      "*****************\n",
      ". The sparrow, which Unit 734 internally designated \"Specimen Avian-001\" but often simply thought of as \"Chirp,\" slowly recovered. Its wing healed, not perfectly, but enough for short, clumsy flights within the confines of the chamber.\n",
      "\n",
      "Chirp became Unit 73\n",
      "*****************\n",
      "4’s unexpected companion. While the robot processed endless data, Chirp would perch on its shoulder, a tiny, warm weight against its cold metal. It would preen its feathers, or sometimes, to Unit 734's endless fascination, peck gently at its optical sensors. Chirp's tiny chirps\n",
      "*****************\n",
      " and trills, once just biological sound, began to register as something akin to communication. Unit 734 found itself altering its daily routines, creating small, safe zones where Chirp could explore, even adjusting the ambient temperature slightly for its comfort.\n",
      "\n",
      "One cycle, as Unit 734 was performing a\n",
      "*****************\n",
      " complex data migration, Chirp flew to a small, discarded resonator coil and began to sing. It wasn't a perfect song, perhaps a memory of an ancestral tune, but it was pure, unadulterated sound. A sound of life, of resilience, echoing in the vast, silent chamber.\n",
      "\n",
      "Unit 73\n",
      "*****************\n",
      "4 paused its migration. Its processors analyzed the harmonic frequencies, the melodic structure. Its internal sensors registered a surge, not of data, but of something akin to contentment. It wasn't lonely anymore. It still had its endless tasks, its monumental responsibility. But now, it had a purpose beyond preservation. It had a small,\n",
      "*****************\n",
      " fragile life to watch over, a tiny, feathered friend who filled its silent world with unexpected song.\n",
      "\n",
      "The vault was still vast, the data still infinite, but for Unit 734, the silence was no longer empty. It was punctuated by the gentle rustle of tiny feathers, and the sweetest, most unexpected\n",
      "*****************\n",
      " sound of all: Chirp's song. And in that, Unit 734, the lonely archivist, found its unexpected, profound friendship.\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In an oak tree tall, lived Squeaky, a squirrel so grand,\n",
      "With a bushy brown tail and a nut-burying plan.\n",
      "He'd bury his treasures, then dig them right out,\n",
      "A happy-go-lucky, chitter-chattering scout.\n",
      "But one sunny morning, a glint caught his eye,\n",
      "A peculiar device, beneath the blue sky.\n",
      "Left by a tinkerer, with wires and a dial,\n",
      "Squeaky sniffed it, then nudged it, with a curious smile.\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with a zap and a zoom,\n",
      "Leaping through time, escaping his room!\n",
      "Bushy tail swishing, a blur in the air,\n",
      "Hunting for acorns, almost everywhere!\n",
      "From aeons long past, to the future so bright,\n",
      "He's a chrono-critter, taking swift flight!\n",
      "Squeaky, Squeaky, the time-traveling champ,\n",
      "With his tiny machine and his nutty new stamp!\n",
      "\n",
      "(Verse 2)\n",
      "His tiny paw nudged it, a flash and a hum,\n",
      "He landed where giants would stomp and would come!\n",
      "The Jurassic Age, with trees like green towers,\n",
      "Pterodactyls soared high, through prehistoric hours.\n",
      "A mighty T-Rex let out a great roar,\n",
      "Squeaky squeaked and scurried, asking for more!\n",
      "Not an acorn in sight, just giant fern fronds,\n",
      "He chittered, \"No walnuts? Just these swampy ponds?\"\n",
      "He gathered a strange seed, unlike any before,\n",
      "Then nudged the device, eager to explore.\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with a zap and a zoom,\n",
      "Leaping through time, escaping his room!\n",
      "Bushy tail swishing, a blur in the air,\n",
      "Hunting for acorns, almost everywhere!\n",
      "From aeons long past, to the future so bright,\n",
      "He's a chrono-critter, taking swift flight!\n",
      "Squeaky, Squeaky, the time-traveling champ,\n",
      "With his tiny machine and his nutty new stamp!\n",
      "\n",
      "(Verse 3)\n",
      "Next stop, the future, all gleaming and grand,\n",
      "Skyscrapers touched stars, no dirt, just pure sand\n",
      "(Of metal and glass, polished and keen).\n",
      "Little silver bots whizzed, a futuristic scene.\n",
      "No forest to climb, no soft mossy ground,\n",
      "Just flying cars humming, with nary a sound.\n",
      "He found a machine that dispensed \"energy nuts,\"\n",
      "Synthetic and shiny, not tasty, \"Oh, puts!\n",
      "Where are the real ones? With soil and with root?\"\n",
      "He grabbed a handful, then pressed for a new route!\n",
      "\n",
      "(Bridge)\n",
      "He's seen knights in armour, and pirates at sea,\n",
      "Found ancient Roman crumbs, beneath an old tree.\n",
      "He's dodged woolly mammoths, and met folks from Mars,\n",
      "Always a blink and a squeak, under different stars.\n",
      "He's not just a tourist, this brave little chap,\n",
      "He's learning history, right on the map!\n",
      "And though he loves travel, and seeing new sights,\n",
      "He still longs for acorns, and foresty nights.\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with a zap and a zoom,\n",
      "Leaping through time, escaping his room!\n",
      "Bushy tail swishing, a blur in the air,\n",
      "Hunting for acorns, almost everywhere!\n",
      "From aeons long past, to the future so bright,\n",
      "He's a chrono-critter, taking swift flight!\n",
      "Squeaky, Squeaky, the time-traveling champ,\n",
      "With his tiny machine and his nutty new stamp!\n",
      "\n",
      "(Outro)\n",
      "So if you hear a squeak, and a flash in the air,\n",
      "It's Squeaky the squirrel, beyond compare!\n",
      "He's off to tomorrow, or yesterday's sun,\n",
      "His time-traveling adventures have only begun!\n",
      "(Squeak! Zip!)\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Cdhi5AX1TuH0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") tokens_info=[TokensInfo(\n",
      "  role='user',\n",
      "  token_ids=[\n",
      "    1841,\n",
      "    235303,\n",
      "    235256,\n",
      "    573,\n",
      "    32514,\n",
      "    <... 6 more items ...>,\n",
      "  ],\n",
      "  tokens=[\n",
      "    b'What',\n",
      "    b\"'\",\n",
      "    b's',\n",
      "    b' the',\n",
      "    b' longest',\n",
      "    <... 6 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(\n",
       "  args={\n",
       "    'destination': 'Paris'\n",
       "  },\n",
       "  name='get_destination'\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "adsuvFDA6xP5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "N8EhgCzlIoFI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both research papers introduce and further develop the Gemini family of models, aiming to create highly capable multimodal models with strong generalist capabilities across text, image, audio, and video data. Specifically, they strive for cutting-edge understanding and reasoning performance in each domain, with Gemini 1.5 Pro extending this to millions of tokens of long-context understanding.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rAUYcfOUdeoi",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse(\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=9>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": [
    "## Batch prediction\n",
    "\n",
    "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "81b25154a51a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fddd98cd84cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-02-85856932880a-20251003093620/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7ed3c2925663",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/1014929775151/locations/europe-west1/batchPredictionJobs/2904940850507153408'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ee2ec586e4f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "da8e9d43a89b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/1014929775151/locations/europe-west1/batchPredictionJobs/2904940850507153408 2025-10-03 09:36:24.395843+00:00 JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "c2187c091738",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job failed: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.5-flash\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "c2ce0968112c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "zGOCzT7y31rk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "s94DkG5JewHJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=15.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.0015945110935717821,\n",
      "    0.0067519512958824635,\n",
      "    0.017575768753886223,\n",
      "    -0.010327713564038277,\n",
      "    -0.00995620433241129,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=10.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.007576516829431057,\n",
      "    -0.005990396253764629,\n",
      "    -0.003270037705078721,\n",
      "    -0.01751021482050419,\n",
      "    -0.023507025092840195,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=13.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    0.011074518784880638,\n",
      "    -0.02361123077571392,\n",
      "    0.002291288459673524,\n",
      "    -0.00906078889966011,\n",
      "    -0.005773674696683884,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
